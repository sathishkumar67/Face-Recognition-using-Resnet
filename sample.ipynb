{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28ba0ea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow==2.19.0 (from -r requirements.txt (line 1))\n",
      "  Using cached tensorflow-2.19.0-cp312-cp312-win_amd64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: mtcnn==1.0.0 in c:\\users\\sathish\\miniconda3\\lib\\site-packages (from -r requirements.txt (line 2)) (1.0.0)\n",
      "Requirement already satisfied: tqdm==4.67.1 in c:\\users\\sathish\\miniconda3\\lib\\site-packages (from -r requirements.txt (line 3)) (4.67.1)\n",
      "Requirement already satisfied: numpy==1.26.4 in c:\\users\\sathish\\miniconda3\\lib\\site-packages (from -r requirements.txt (line 4)) (1.26.4)\n",
      "Requirement already satisfied: opencv-python==4.11.0.86 in c:\\users\\sathish\\miniconda3\\lib\\site-packages (from -r requirements.txt (line 5)) (4.11.0.86)\n",
      "Requirement already satisfied: huggingface-hub==0.30.2 in c:\\users\\sathish\\miniconda3\\lib\\site-packages (from -r requirements.txt (line 6)) (0.30.2)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow==2.19.0->-r requirements.txt (line 1))\n",
      "  Using cached absl_py-2.2.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow==2.19.0->-r requirements.txt (line 1))\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow==2.19.0->-r requirements.txt (line 1))\n",
      "  Using cached flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow==2.19.0->-r requirements.txt (line 1))\n",
      "  Using cached gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow==2.19.0->-r requirements.txt (line 1))\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting libclang>=13.0.0 (from tensorflow==2.19.0->-r requirements.txt (line 1))\n",
      "  Using cached libclang-18.1.1-py2.py3-none-win_amd64.whl.metadata (5.3 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow==2.19.0->-r requirements.txt (line 1))\n",
      "  Using cached opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\sathish\\miniconda3\\lib\\site-packages (from tensorflow==2.19.0->-r requirements.txt (line 1)) (24.1)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 (from tensorflow==2.19.0->-r requirements.txt (line 1))\n",
      "  Using cached protobuf-5.29.4-cp310-abi3-win_amd64.whl.metadata (592 bytes)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\sathish\\miniconda3\\lib\\site-packages (from tensorflow==2.19.0->-r requirements.txt (line 1)) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\sathish\\miniconda3\\lib\\site-packages (from tensorflow==2.19.0->-r requirements.txt (line 1)) (75.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\sathish\\miniconda3\\lib\\site-packages (from tensorflow==2.19.0->-r requirements.txt (line 1)) (1.17.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow==2.19.0->-r requirements.txt (line 1))\n",
      "  Using cached termcolor-3.1.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\sathish\\miniconda3\\lib\\site-packages (from tensorflow==2.19.0->-r requirements.txt (line 1)) (4.13.2)\n",
      "Collecting wrapt>=1.11.0 (from tensorflow==2.19.0->-r requirements.txt (line 1))\n",
      "  Using cached wrapt-1.17.2-cp312-cp312-win_amd64.whl.metadata (6.5 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow==2.19.0->-r requirements.txt (line 1))\n",
      "  Using cached grpcio-1.71.0-cp312-cp312-win_amd64.whl.metadata (4.0 kB)\n",
      "Collecting tensorboard~=2.19.0 (from tensorflow==2.19.0->-r requirements.txt (line 1))\n",
      "  Using cached tensorboard-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting keras>=3.5.0 (from tensorflow==2.19.0->-r requirements.txt (line 1))\n",
      "  Using cached keras-3.9.2-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting h5py>=3.11.0 (from tensorflow==2.19.0->-r requirements.txt (line 1))\n",
      "  Using cached h5py-3.13.0-cp312-cp312-win_amd64.whl.metadata (2.5 kB)\n",
      "Collecting ml-dtypes<1.0.0,>=0.5.1 (from tensorflow==2.19.0->-r requirements.txt (line 1))\n",
      "  Using cached ml_dtypes-0.5.1-cp312-cp312-win_amd64.whl.metadata (22 kB)\n",
      "Requirement already satisfied: joblib>=1.4.2 in c:\\users\\sathish\\miniconda3\\lib\\site-packages (from mtcnn==1.0.0->-r requirements.txt (line 2)) (1.4.2)\n",
      "Requirement already satisfied: lz4>=4.3.3 in c:\\users\\sathish\\miniconda3\\lib\\site-packages (from mtcnn==1.0.0->-r requirements.txt (line 2)) (4.4.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\sathish\\miniconda3\\lib\\site-packages (from tqdm==4.67.1->-r requirements.txt (line 3)) (0.4.6)\n",
      "Requirement already satisfied: filelock in c:\\users\\sathish\\miniconda3\\lib\\site-packages (from huggingface-hub==0.30.2->-r requirements.txt (line 6)) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\sathish\\miniconda3\\lib\\site-packages (from huggingface-hub==0.30.2->-r requirements.txt (line 6)) (2025.3.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\sathish\\miniconda3\\lib\\site-packages (from huggingface-hub==0.30.2->-r requirements.txt (line 6)) (6.0.2)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\sathish\\miniconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow==2.19.0->-r requirements.txt (line 1)) (0.44.0)\n",
      "Collecting rich (from keras>=3.5.0->tensorflow==2.19.0->-r requirements.txt (line 1))\n",
      "  Using cached rich-14.0.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting namex (from keras>=3.5.0->tensorflow==2.19.0->-r requirements.txt (line 1))\n",
      "  Using cached namex-0.0.9-py3-none-any.whl.metadata (322 bytes)\n",
      "Collecting optree (from keras>=3.5.0->tensorflow==2.19.0->-r requirements.txt (line 1))\n",
      "  Using cached optree-0.15.0-cp312-cp312-win_amd64.whl.metadata (49 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sathish\\miniconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow==2.19.0->-r requirements.txt (line 1)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sathish\\miniconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow==2.19.0->-r requirements.txt (line 1)) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sathish\\miniconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow==2.19.0->-r requirements.txt (line 1)) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sathish\\miniconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow==2.19.0->-r requirements.txt (line 1)) (2025.1.31)\n",
      "Collecting markdown>=2.6.8 (from tensorboard~=2.19.0->tensorflow==2.19.0->-r requirements.txt (line 1))\n",
      "  Using cached markdown-3.8-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard~=2.19.0->tensorflow==2.19.0->-r requirements.txt (line 1))\n",
      "  Using cached tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard~=2.19.0->tensorflow==2.19.0->-r requirements.txt (line 1))\n",
      "  Using cached werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting MarkupSafe>=2.1.1 (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow==2.19.0->-r requirements.txt (line 1))\n",
      "  Using cached MarkupSafe-3.0.2-cp312-cp312-win_amd64.whl.metadata (4.1 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->keras>=3.5.0->tensorflow==2.19.0->-r requirements.txt (line 1))\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\sathish\\miniconda3\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow==2.19.0->-r requirements.txt (line 1)) (2.19.1)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow==2.19.0->-r requirements.txt (line 1))\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Using cached tensorflow-2.19.0-cp312-cp312-win_amd64.whl (376.0 MB)\n",
      "Using cached absl_py-2.2.2-py3-none-any.whl (135 kB)\n",
      "Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Using cached flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
      "Using cached gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Downloading grpcio-1.71.0-cp312-cp312-win_amd64.whl (4.3 MB)\n",
      "   ---------------------------------------- 0.0/4.3 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 0.8/4.3 MB 4.8 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 1.8/4.3 MB 4.8 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 2.6/4.3 MB 4.9 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 3.9/4.3 MB 4.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 4.3/4.3 MB 4.6 MB/s eta 0:00:00\n",
      "Downloading h5py-3.13.0-cp312-cp312-win_amd64.whl (3.0 MB)\n",
      "   ---------------------------------------- 0.0/3.0 MB ? eta -:--:--\n",
      "   ---------- ----------------------------- 0.8/3.0 MB 4.8 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 1.8/3.0 MB 4.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.9/3.0 MB 4.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.0/3.0 MB 4.5 MB/s eta 0:00:00\n",
      "Using cached keras-3.9.2-py3-none-any.whl (1.3 MB)\n",
      "Using cached libclang-18.1.1-py2.py3-none-win_amd64.whl (26.4 MB)\n",
      "Downloading ml_dtypes-0.5.1-cp312-cp312-win_amd64.whl (210 kB)\n",
      "Using cached opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Downloading protobuf-5.29.4-cp310-abi3-win_amd64.whl (434 kB)\n",
      "Using cached tensorboard-2.19.0-py3-none-any.whl (5.5 MB)\n",
      "Downloading termcolor-3.1.0-py3-none-any.whl (7.7 kB)\n",
      "Downloading wrapt-1.17.2-cp312-cp312-win_amd64.whl (38 kB)\n",
      "Downloading markdown-3.8-py3-none-any.whl (106 kB)\n",
      "Using cached tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Using cached werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "Downloading namex-0.0.9-py3-none-any.whl (5.8 kB)\n",
      "Downloading optree-0.15.0-cp312-cp312-win_amd64.whl (307 kB)\n",
      "Downloading rich-14.0.0-py3-none-any.whl (243 kB)\n",
      "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading MarkupSafe-3.0.2-cp312-cp312-win_amd64.whl (15 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: namex, libclang, flatbuffers, wrapt, termcolor, tensorboard-data-server, protobuf, optree, opt-einsum, ml-dtypes, mdurl, MarkupSafe, markdown, h5py, grpcio, google-pasta, gast, astunparse, absl-py, werkzeug, markdown-it-py, tensorboard, rich, keras, tensorflow\n",
      "Successfully installed MarkupSafe-3.0.2 absl-py-2.2.2 astunparse-1.6.3 flatbuffers-25.2.10 gast-0.6.0 google-pasta-0.2.0 grpcio-1.71.0 h5py-3.13.0 keras-3.9.2 libclang-18.1.1 markdown-3.8 markdown-it-py-3.0.0 mdurl-0.1.2 ml-dtypes-0.5.1 namex-0.0.9 opt-einsum-3.4.0 optree-0.15.0 protobuf-5.29.4 rich-14.0.0 tensorboard-2.19.0 tensorboard-data-server-0.7.2 tensorflow-2.19.0 termcolor-3.1.0 werkzeug-3.1.3 wrapt-1.17.2\n"
     ]
    }
   ],
   "source": [
    "# !git clone https://github.com/sathishkumar67/Face-Recognition-using-Resnet.git\n",
    "# !mv /kaggle/working/Face-Recognition-using-Resnet/* /kaggle/working/\n",
    "!pip install -r requirements.txt\n",
    "# !pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56e1ad87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sathish\\miniconda3\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import cv2\n",
    "from mtcnn import MTCNN\n",
    "import numpy as np\n",
    "from huggingface_hub import hf_hub_download\n",
    "from siamese_resnet import unzip_file\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# from torch.utils.data import Dataset\n",
    "# import torchvision\n",
    "# import torchvision.transforms as transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87696724",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_REPO_ID = \"pt-sk/Face_Recognition_Dataset\"\n",
    "DATASET_FILENAME_IN_REPO = \"Face Recognition Dataset.zip\"\n",
    "DATASET_REPO_TYPE = \"dataset\"\n",
    "LOCAL_DIR = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2fe865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the dataset from Hugging Face Hub\n",
    "hf_hub_download(repo_id=DATASET_REPO_ID, filename=DATASET_FILENAME_IN_REPO, repo_type=DATASET_REPO_TYPE, local_dir=LOCAL_DIR)\n",
    "\n",
    "# Unzip the dataset\n",
    "unzip_file(os.path.join(LOCAL_DIR, DATASET_FILENAME_IN_REPO), LOCAL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87986242",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1746252743.278495     161 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15513 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing train set:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_161/151218057.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;31m# Example usage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m     split_dataset(\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0mroot_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Face Recognition Dataset\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0msave_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"processed_data\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_161/151218057.py\u001b[0m in \u001b[0;36msplit_dataset\u001b[0;34m(root_dir, save_dir, train_ratio, val_ratio, test_ratio, target_size)\u001b[0m\n\u001b[1;32m     58\u001b[0m                             dynamic_ncols=True)\n\u001b[1;32m     59\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mimage_path\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimage_pbar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m                 result = _process_and_save_image(\n\u001b[0m\u001b[1;32m     61\u001b[0m                     \u001b[0mimage_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m                     \u001b[0moutput_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_161/151218057.py\u001b[0m in \u001b[0;36m_process_and_save_image\u001b[0;34m(image_path, output_dir, detector, target_size)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;31m# Detect faces\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         \u001b[0mfaces\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetect_faces\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfaces\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m\"skipped\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mtcnn/mtcnn.py\u001b[0m in \u001b[0;36mdetect_faces\u001b[0;34m(self, image, fit_to_image, limit_boundaries_landmarks, box_format, output_type, postprocess, batch_stack_justification, **kwargs)\u001b[0m\n\u001b[1;32m    157\u001b[0m                 \u001b[0;31m# Process images through each stage (PNet, RNet, ONet)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mstage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m                     \u001b[0mbboxes_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbboxes_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbboxes_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages_normalized\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimages_normalized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages_oshapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimages_oshapes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# No faces found\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mtcnn/stages/stage_pnet.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, images_normalized, images_oshapes, min_face_size, min_size, scale_factor, threshold_pnet, nms_pnet1, nms_pnet2, **kwargs)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;31m# 4. Generate bounding boxes per scale group\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0mbboxes_proposals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgenerate_bounding_box\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold_pnet\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpnet_result\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0mbboxes_batch_upscaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mupscale_bboxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbboxes_proposals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscales_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mtcnn/stages/stage_pnet.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;31m# 4. Generate bounding boxes per scale group\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0mbboxes_proposals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgenerate_bounding_box\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold_pnet\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpnet_result\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0mbboxes_batch_upscaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mupscale_bboxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbboxes_proposals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscales_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mtcnn/utils/bboxes.py\u001b[0m in \u001b[0;36mgenerate_bounding_box\u001b[0;34m(bbox_reg, bbox_class, threshold_face, strides, cell_size)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;31m# Concatenate the bounding box coordinates and detection information, keeping batch index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m     bboxes_result = np.stack([\n\u001b[0m\u001b[1;32m     81\u001b[0m         \u001b[0mindex_bboxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfidence_score\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex_bboxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_bboxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_bboxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     ], axis=0).T\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36m_stack_dispatcher\u001b[0;34m(arrays, axis, out, dtype, casting)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m def _stack_dispatcher(arrays, axis=None, out=None, *,\n\u001b[0m\u001b[1;32m    363\u001b[0m                       dtype=None, casting=None):\n\u001b[1;32m    364\u001b[0m     \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_arrays_for_stack_dispatcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def split_dataset(\n",
    "    root_dir: str,\n",
    "    save_dir: str,\n",
    "    train_ratio: float = 0.8,\n",
    "    val_ratio: float = 0.1,\n",
    "    test_ratio: float = 0.1,\n",
    "    target_size: tuple = (224, 224)) -> None:\n",
    "    \"\"\"\n",
    "    Splits a facial image dataset into train/val/test sets, detects and crops faces,\n",
    "    and saves processed images in organized directories.\n",
    "\n",
    "    Args:\n",
    "        root_dir: Directory containing identity subfolders with images\n",
    "        save_dir: Directory to save processed images\n",
    "        train_ratio: Proportion of data for training\n",
    "        val_ratio: Proportion of data for validation\n",
    "        test_ratio: Proportion of data for testing\n",
    "        target_size: Size to resize detected faces to\n",
    "    \"\"\"\n",
    "    \n",
    "    # Validate input ratios\n",
    "    assert abs((train_ratio + val_ratio + test_ratio) - 1.0) < 1e-9, \"Ratios must sum to 1.0\"\n",
    "    \n",
    "    # Initialize face detector\n",
    "    face_detector = MTCNN()\n",
    "\n",
    "    # Get and shuffle valid identities\n",
    "    identities = _get_valid_identities(root_dir)\n",
    "    random.shuffle(identities)\n",
    "\n",
    "    # Split identities into groups\n",
    "    split_groups = _split_identities(identities, train_ratio, val_ratio, test_ratio)\n",
    "    splits = [\"train\", \"val\", \"test\"]\n",
    "\n",
    "    # Process each split group\n",
    "    for split_name, identity_group in zip(splits, split_groups):\n",
    "        print(f\"\\nProcessing {split_name} set:\")\n",
    "        processed_count = 0\n",
    "        skipped_images = 0\n",
    "\n",
    "        # Main progress bar for identities\n",
    "        identity_pbar = tqdm(identity_group, desc=\"Identities\", leave=False)\n",
    "        for identity in identity_pbar:\n",
    "            identity_path = os.path.join(root_dir, identity)\n",
    "            image_paths = _get_image_paths(identity_path)\n",
    "            \n",
    "            # Update identity progress bar description\n",
    "            identity_pbar.set_postfix_str(f\"Processing: {identity[:15]}...\")\n",
    "            \n",
    "            # Create output directory for this identity\n",
    "            output_dir = os.path.join(save_dir, split_name, identity)\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "            # Images progress bar for current identity\n",
    "            image_pbar = tqdm(image_paths, \n",
    "                            desc=f\"Images ({identity[:12]}...)\", \n",
    "                            leave=False,\n",
    "                            dynamic_ncols=True)\n",
    "            for image_path in image_pbar:\n",
    "                result = _process_and_save_image(\n",
    "                    image_path=image_path,\n",
    "                    output_dir=output_dir,\n",
    "                    detector=face_detector,\n",
    "                    target_size=target_size\n",
    "                )\n",
    "                \n",
    "                if result == \"skipped\":\n",
    "                    skipped_images += 1\n",
    "                else:\n",
    "                    processed_count += 1\n",
    "                \n",
    "                # Update image progress bar postfix\n",
    "                image_pbar.set_postfix_str(f\"Saved: {processed_count} | Skipped: {skipped_images}\")\n",
    "            \n",
    "            image_pbar.close()\n",
    "\n",
    "        identity_pbar.close()\n",
    "\n",
    "        # Print statistics\n",
    "        _print_split_stats(split_name, processed_count, skipped_images, save_dir)\n",
    "\n",
    "    # Cleanup original data\n",
    "    shutil.rmtree(root_dir)\n",
    "    print(\"\\n✅ Original dataset directory removed.\")\n",
    "def _get_valid_identities(root_dir: str) -> list:\n",
    "    \"\"\"Get identities with valid image directories\"\"\"\n",
    "    return [\n",
    "        identity for identity in os.listdir(root_dir)\n",
    "        if os.path.isdir(os.path.join(root_dir, identity))\n",
    "    ]\n",
    "\n",
    "def _split_identities(\n",
    "    identities: list,\n",
    "    train_ratio: float,\n",
    "    val_ratio: float,\n",
    "    test_ratio: float)-> tuple:\n",
    "    \"\"\"Split identities into train/val/test groups\"\"\"\n",
    "    total = len(identities)\n",
    "    train_end = int(train_ratio * total)\n",
    "    val_end = train_end + int(val_ratio * total)\n",
    "    \n",
    "    return (\n",
    "        identities[:train_end],\n",
    "        identities[train_end:val_end],\n",
    "        identities[val_end:]\n",
    "    )\n",
    "\n",
    "def _get_image_paths(identity_path: str) -> list:\n",
    "    \"\"\"Get valid image paths for an identity\"\"\"\n",
    "    return [\n",
    "        os.path.join(identity_path, img)\n",
    "        for img in os.listdir(identity_path)\n",
    "        if img.lower().endswith(('.jpg', '.png', '.jpeg'))\n",
    "    ]\n",
    "\n",
    "def _process_and_save_image(\n",
    "    image_path: str,\n",
    "    output_dir: str,\n",
    "    detector: MTCNN,\n",
    "    target_size: tuple)-> str:\n",
    "    \"\"\"Process single image and save cropped face\"\"\"\n",
    "    try:\n",
    "        # Read and convert image\n",
    "        image = cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Detect faces\n",
    "        faces = detector.detect_faces(image)\n",
    "        if not faces:\n",
    "            return \"skipped\"\n",
    "\n",
    "        # Select best face\n",
    "        best_face = max(faces, key=lambda x: x['confidence'])\n",
    "        x, y, w, h = best_face['box']\n",
    "        \n",
    "        # Crop and resize\n",
    "        face_region = image[y:y+h, x:x+w]\n",
    "        resized_face = cv2.resize(face_region, target_size, interpolation=cv2.INTER_LANCZOS4)\n",
    "        \n",
    "        # Save image\n",
    "        filename = f\"{os.path.splitext(os.path.basename(image_path))[0]}_cropped.jpg\"\n",
    "        output_path = os.path.join(output_dir, filename)\n",
    "        cv2.imwrite(output_path, cv2.cvtColor(resized_face, cv2.COLOR_RGB2BGR))\n",
    "        \n",
    "        return \"saved\"\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error processing {image_path}: {str(e)}\")\n",
    "        return \"skipped\"\n",
    "\n",
    "def _print_split_stats(split_name: str, valid_count: int, skipped_count: int, save_dir: str) -> None:\n",
    "    \"\"\"Print statistics for processed split\"\"\"\n",
    "    print(f\"\\n{split_name.capitalize()} Set Summary:\")\n",
    "    print(f\"✅ Successfully saved faces: {valid_count}\")\n",
    "    print(f\"⏭️ Skipped images (no face detected/errors): {skipped_count}\")\n",
    "    print(f\"📊 Total images processed: {valid_count + skipped_count}\")\n",
    "    print(f\"📁 Output directory: {os.path.abspath(save_dir)}/{split_name}\\n\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    split_dataset(\n",
    "        root_dir=\"Face Recognition Dataset\",\n",
    "        save_dir=\"processed_data\",\n",
    "        train_ratio=0.8,\n",
    "        val_ratio=0.1,\n",
    "        test_ratio=0.1,\n",
    "        target_size=(224, 224)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267d212c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73943b0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f1a96d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1214dbd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e255651a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6f96b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eea591f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6b8f82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d75d1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c3bb56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604f6af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# triplet loss function\n",
    "# This function computes the triplet loss for a batch of anchor, positive, and negative samples.\n",
    "class TripletLoss(nn.Module):\n",
    "    def __init__(self, margin=0.5):\n",
    "        super(TripletLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, anchor, positive, negative):\n",
    "        # Compute pairwise distances\n",
    "        distance_positive = F.pairwise_distance(anchor, positive)\n",
    "        distance_negative = F.pairwise_distance(anchor, negative)\n",
    "        \n",
    "        # Calculate triplet loss\n",
    "        losses = F.relu(distance_positive - distance_negative + self.margin)\n",
    "        return losses.mean()\n",
    "    \n",
    "class SiameseResNet(nn.Module):\n",
    "    def __init__(self, embedding_dim=256):\n",
    "        super(SiameseResNet, self).__init__()\n",
    "        # Load pretrained ResNet18\n",
    "        self.backbone = torchvision.models.resnet18(weights=\"IMAGENET1K_V1\", progress=True)\n",
    "        \n",
    "        # Replace the final fully connected layer\n",
    "        self.backbone.fc = nn.Linear(512, embedding_dim)  # 512 -> 256\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)\n",
    "    \n",
    "    def print_parameters_count(self):\n",
    "        total_params = sum(p.numel() for p in self.parameters()) / 1e6  # Convert to millions\n",
    "        # Print the number of parameters in millions\n",
    "        print(f\"Total parameters: {total_params:.2f}m\")\n",
    "        \n",
    "model = SiameseResNet(embedding_dim=256)\n",
    "model.print_parameters_count()\n",
    "\n",
    "# pass a sample image through the model\n",
    "dummy_input = torch.randn(1, 3, 224, 224)  # batch size of 1, 3 channels, 224x224 image\n",
    "output = model(dummy_input)\n",
    "output.shape  # should be (1, 256) since we changed the final layer to output 256 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60f3e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "# Define transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(100),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Initialize dataset and dataloader\n",
    "dataset = TripletFaceDataset(root_dir=\"path/to/dataset\", transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Initialize model, loss, and optimizer\n",
    "model = SiameseResNet(embedding_dim=256)\n",
    "criterion = TripletLoss(margin=0.5)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "# Training loop\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(10):\n",
    "    for batch in dataloader:\n",
    "        anchor, positive, negative = batch\n",
    "        anchor, positive, negative = anchor.to(device), positive.to(device), negative.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        anchor_emb = model(anchor)\n",
    "        positive_emb = model(positive)\n",
    "        negative_emb = model(negative)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(anchor_emb, positive_emb, negative_emb)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82942de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3742f799",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
