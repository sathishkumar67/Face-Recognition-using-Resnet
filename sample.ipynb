{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28ba0ea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mtcnn==1.0.0 (from -r requirements.txt (line 1))\n",
      "  Downloading mtcnn-1.0.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting tqdm==4.67.1 (from -r requirements.txt (line 2))\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting numpy==1.26.4 (from -r requirements.txt (line 3))\n",
      "  Downloading numpy-1.26.4-cp312-cp312-win_amd64.whl.metadata (61 kB)\n",
      "Collecting opencv-python==4.11.0.86 (from -r requirements.txt (line 4))\n",
      "  Using cached opencv_python-4.11.0.86-cp37-abi3-win_amd64.whl.metadata (20 kB)\n",
      "Collecting huggingface-hub==0.30.2 (from -r requirements.txt (line 5))\n",
      "  Downloading huggingface_hub-0.30.2-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting joblib>=1.4.2 (from mtcnn==1.0.0->-r requirements.txt (line 1))\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting lz4>=4.3.3 (from mtcnn==1.0.0->-r requirements.txt (line 1))\n",
      "  Downloading lz4-4.4.4-cp312-cp312-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\sathish\\miniconda3\\lib\\site-packages (from tqdm==4.67.1->-r requirements.txt (line 2)) (0.4.6)\n",
      "Collecting filelock (from huggingface-hub==0.30.2->-r requirements.txt (line 5))\n",
      "  Downloading filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub==0.30.2->-r requirements.txt (line 5))\n",
      "  Downloading fsspec-2025.3.2-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\sathish\\miniconda3\\lib\\site-packages (from huggingface-hub==0.30.2->-r requirements.txt (line 5)) (24.1)\n",
      "Collecting pyyaml>=5.1 (from huggingface-hub==0.30.2->-r requirements.txt (line 5))\n",
      "  Using cached PyYAML-6.0.2-cp312-cp312-win_amd64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\sathish\\miniconda3\\lib\\site-packages (from huggingface-hub==0.30.2->-r requirements.txt (line 5)) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\sathish\\miniconda3\\lib\\site-packages (from huggingface-hub==0.30.2->-r requirements.txt (line 5)) (4.13.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sathish\\miniconda3\\lib\\site-packages (from requests->huggingface-hub==0.30.2->-r requirements.txt (line 5)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sathish\\miniconda3\\lib\\site-packages (from requests->huggingface-hub==0.30.2->-r requirements.txt (line 5)) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sathish\\miniconda3\\lib\\site-packages (from requests->huggingface-hub==0.30.2->-r requirements.txt (line 5)) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sathish\\miniconda3\\lib\\site-packages (from requests->huggingface-hub==0.30.2->-r requirements.txt (line 5)) (2025.1.31)\n",
      "Downloading mtcnn-1.0.0-py3-none-any.whl (1.9 MB)\n",
      "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "   ---------------- ----------------------- 0.8/1.9 MB 4.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 1.8/1.9 MB 4.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.9/1.9 MB 4.4 MB/s eta 0:00:00\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading numpy-1.26.4-cp312-cp312-win_amd64.whl (15.5 MB)\n",
      "   ---------------------------------------- 0.0/15.5 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 1.0/15.5 MB 5.0 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 2.1/15.5 MB 4.9 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 3.1/15.5 MB 4.7 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 3.9/15.5 MB 4.8 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 5.0/15.5 MB 4.6 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 5.8/15.5 MB 4.6 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 6.8/15.5 MB 4.7 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 7.9/15.5 MB 4.6 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 8.9/15.5 MB 4.7 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 10.0/15.5 MB 4.6 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 11.0/15.5 MB 4.6 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 12.1/15.5 MB 4.7 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 12.8/15.5 MB 4.7 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 13.9/15.5 MB 4.7 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 14.9/15.5 MB 4.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  15.5/15.5 MB 4.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 15.5/15.5 MB 4.4 MB/s eta 0:00:00\n",
      "Using cached opencv_python-4.11.0.86-cp37-abi3-win_amd64.whl (39.5 MB)\n",
      "Downloading huggingface_hub-0.30.2-py3-none-any.whl (481 kB)\n",
      "Downloading fsspec-2025.3.2-py3-none-any.whl (194 kB)\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Downloading lz4-4.4.4-cp312-cp312-win_amd64.whl (99 kB)\n",
      "Downloading PyYAML-6.0.2-cp312-cp312-win_amd64.whl (156 kB)\n",
      "Downloading filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Installing collected packages: tqdm, pyyaml, numpy, lz4, joblib, fsspec, filelock, opencv-python, mtcnn, huggingface-hub\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.66.5\n",
      "    Uninstalling tqdm-4.66.5:\n",
      "      Successfully uninstalled tqdm-4.66.5\n",
      "Successfully installed filelock-3.18.0 fsspec-2025.3.2 huggingface-hub-0.30.2 joblib-1.4.2 lz4-4.4.4 mtcnn-1.0.0 numpy-1.26.4 opencv-python-4.11.0.86 pyyaml-6.0.2 tqdm-4.67.1\n"
     ]
    }
   ],
   "source": [
    "# !git clone https://github.com/sathishkumar67/Face-Recognition-using-Resnet.git\n",
    "# !mv /kaggle/working/Face-Recognition-using-Resnet/* /kaggle/working/\n",
    "!pip install -r requirements.txt\n",
    "# !pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56e1ad87",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcv2\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# from collections import defaultdict\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmtcnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MTCNN\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# import torch\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# import torch.nn as nn\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# import torch.nn.functional as F\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# import torchvision\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# from torch.utils.data import Dataset\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sathish\\miniconda3\\Lib\\site-packages\\mtcnn\\__init__.py:23\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# MIT License\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Copyright (c) 2019-2024 IvÃ¡n de Paz Centeno\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# SOFTWARE.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmtcnn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmtcnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MTCNN\n\u001b[32m     25\u001b[39m __all__ = [\u001b[33m\"\u001b[39m\u001b[33mMTCNN\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sathish\\miniconda3\\Lib\\site-packages\\mtcnn\\mtcnn.py:23\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# MIT License\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Copyright (c) 2019-2024 IvÃ¡n de Paz Centeno\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# SOFTWARE.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtf\u001b[39;00m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmtcnn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstages\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StagePNet, StageRNet, StageONet\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import cv2\n",
    "# from collections import defaultdict\n",
    "from mtcnn import MTCNN\n",
    "import numpy as np\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# import torchvision\n",
    "# from torch.utils.data import Dataset\n",
    "from huggingface_hub import hf_hub_download\n",
    "from siamese_resnet import unzip_file\n",
    "# import torchvision.transforms as transforms\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87696724",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_REPO_ID = \"pt-sk/Face_Recognition_Dataset\"\n",
    "DATASET_FILENAME_IN_REPO = \"Face Recognition Dataset.zip\"\n",
    "DATASET_REPO_TYPE = \"dataset\"\n",
    "LOCAL_DIR = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd2fe865",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5d343568e0d4cd59eb191de592cac9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Face%20Recognition%20Dataset.zip:   0%|          | 0.00/5.10G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unzipping: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5.20G/5.20G [00:43<00:00, 119MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unzipped /kaggle/working/Face Recognition Dataset.zip to /kaggle/working\n",
      "Removed zip file: /kaggle/working/Face Recognition Dataset.zip\n"
     ]
    }
   ],
   "source": [
    "# Download the dataset from Hugging Face Hub\n",
    "hf_hub_download(repo_id=DATASET_REPO_ID, filename=DATASET_FILENAME_IN_REPO, repo_type=DATASET_REPO_TYPE, local_dir=LOCAL_DIR)\n",
    "\n",
    "# Unzip the dataset\n",
    "unzip_file(os.path.join(LOCAL_DIR, DATASET_FILENAME_IN_REPO), LOCAL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87986242",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1746252743.278495     161 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15513 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing train set:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_161/151218057.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;31m# Example usage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m     split_dataset(\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0mroot_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Face Recognition Dataset\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0msave_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"processed_data\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_161/151218057.py\u001b[0m in \u001b[0;36msplit_dataset\u001b[0;34m(root_dir, save_dir, train_ratio, val_ratio, test_ratio, target_size)\u001b[0m\n\u001b[1;32m     58\u001b[0m                             dynamic_ncols=True)\n\u001b[1;32m     59\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mimage_path\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimage_pbar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m                 result = _process_and_save_image(\n\u001b[0m\u001b[1;32m     61\u001b[0m                     \u001b[0mimage_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m                     \u001b[0moutput_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_161/151218057.py\u001b[0m in \u001b[0;36m_process_and_save_image\u001b[0;34m(image_path, output_dir, detector, target_size)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;31m# Detect faces\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         \u001b[0mfaces\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetect_faces\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfaces\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m\"skipped\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mtcnn/mtcnn.py\u001b[0m in \u001b[0;36mdetect_faces\u001b[0;34m(self, image, fit_to_image, limit_boundaries_landmarks, box_format, output_type, postprocess, batch_stack_justification, **kwargs)\u001b[0m\n\u001b[1;32m    157\u001b[0m                 \u001b[0;31m# Process images through each stage (PNet, RNet, ONet)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mstage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m                     \u001b[0mbboxes_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbboxes_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbboxes_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages_normalized\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimages_normalized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages_oshapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimages_oshapes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# No faces found\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mtcnn/stages/stage_pnet.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, images_normalized, images_oshapes, min_face_size, min_size, scale_factor, threshold_pnet, nms_pnet1, nms_pnet2, **kwargs)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;31m# 4. Generate bounding boxes per scale group\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0mbboxes_proposals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgenerate_bounding_box\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold_pnet\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpnet_result\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0mbboxes_batch_upscaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mupscale_bboxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbboxes_proposals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscales_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mtcnn/stages/stage_pnet.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;31m# 4. Generate bounding boxes per scale group\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0mbboxes_proposals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgenerate_bounding_box\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold_pnet\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpnet_result\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0mbboxes_batch_upscaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mupscale_bboxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbboxes_proposals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscales_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mtcnn/utils/bboxes.py\u001b[0m in \u001b[0;36mgenerate_bounding_box\u001b[0;34m(bbox_reg, bbox_class, threshold_face, strides, cell_size)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;31m# Concatenate the bounding box coordinates and detection information, keeping batch index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m     bboxes_result = np.stack([\n\u001b[0m\u001b[1;32m     81\u001b[0m         \u001b[0mindex_bboxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfidence_score\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex_bboxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_bboxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_bboxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     ], axis=0).T\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36m_stack_dispatcher\u001b[0;34m(arrays, axis, out, dtype, casting)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m def _stack_dispatcher(arrays, axis=None, out=None, *,\n\u001b[0m\u001b[1;32m    363\u001b[0m                       dtype=None, casting=None):\n\u001b[1;32m    364\u001b[0m     \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_arrays_for_stack_dispatcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def split_dataset(\n",
    "    root_dir: str,\n",
    "    save_dir: str,\n",
    "    train_ratio: float = 0.8,\n",
    "    val_ratio: float = 0.1,\n",
    "    test_ratio: float = 0.1,\n",
    "    target_size: tuple = (224, 224)) -> None:\n",
    "    \"\"\"\n",
    "    Splits a facial image dataset into train/val/test sets, detects and crops faces,\n",
    "    and saves processed images in organized directories.\n",
    "\n",
    "    Args:\n",
    "        root_dir: Directory containing identity subfolders with images\n",
    "        save_dir: Directory to save processed images\n",
    "        train_ratio: Proportion of data for training\n",
    "        val_ratio: Proportion of data for validation\n",
    "        test_ratio: Proportion of data for testing\n",
    "        target_size: Size to resize detected faces to\n",
    "    \"\"\"\n",
    "    \n",
    "    # Validate input ratios\n",
    "    assert abs((train_ratio + val_ratio + test_ratio) - 1.0) < 1e-9, \"Ratios must sum to 1.0\"\n",
    "    \n",
    "    # Initialize face detector\n",
    "    face_detector = MTCNN()\n",
    "\n",
    "    # Get and shuffle valid identities\n",
    "    identities = _get_valid_identities(root_dir)\n",
    "    random.shuffle(identities)\n",
    "\n",
    "    # Split identities into groups\n",
    "    split_groups = _split_identities(identities, train_ratio, val_ratio, test_ratio)\n",
    "    splits = [\"train\", \"val\", \"test\"]\n",
    "\n",
    "    # Process each split group\n",
    "    for split_name, identity_group in zip(splits, split_groups):\n",
    "        print(f\"\\nProcessing {split_name} set:\")\n",
    "        processed_count = 0\n",
    "        skipped_images = 0\n",
    "\n",
    "        # Main progress bar for identities\n",
    "        identity_pbar = tqdm(identity_group, desc=\"Identities\", leave=False)\n",
    "        for identity in identity_pbar:\n",
    "            identity_path = os.path.join(root_dir, identity)\n",
    "            image_paths = _get_image_paths(identity_path)\n",
    "            \n",
    "            # Update identity progress bar description\n",
    "            identity_pbar.set_postfix_str(f\"Processing: {identity[:15]}...\")\n",
    "            \n",
    "            # Create output directory for this identity\n",
    "            output_dir = os.path.join(save_dir, split_name, identity)\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "            # Images progress bar for current identity\n",
    "            image_pbar = tqdm(image_paths, \n",
    "                            desc=f\"Images ({identity[:12]}...)\", \n",
    "                            leave=False,\n",
    "                            dynamic_ncols=True)\n",
    "            for image_path in image_pbar:\n",
    "                result = _process_and_save_image(\n",
    "                    image_path=image_path,\n",
    "                    output_dir=output_dir,\n",
    "                    detector=face_detector,\n",
    "                    target_size=target_size\n",
    "                )\n",
    "                \n",
    "                if result == \"skipped\":\n",
    "                    skipped_images += 1\n",
    "                else:\n",
    "                    processed_count += 1\n",
    "                \n",
    "                # Update image progress bar postfix\n",
    "                image_pbar.set_postfix_str(f\"Saved: {processed_count} | Skipped: {skipped_images}\")\n",
    "            \n",
    "            image_pbar.close()\n",
    "\n",
    "        identity_pbar.close()\n",
    "\n",
    "        # Print statistics\n",
    "        _print_split_stats(split_name, processed_count, skipped_images, save_dir)\n",
    "\n",
    "    # Cleanup original data\n",
    "    shutil.rmtree(root_dir)\n",
    "    print(\"\\nâœ… Original dataset directory removed.\")\n",
    "def _get_valid_identities(root_dir: str) -> list:\n",
    "    \"\"\"Get identities with valid image directories\"\"\"\n",
    "    return [\n",
    "        identity for identity in os.listdir(root_dir)\n",
    "        if os.path.isdir(os.path.join(root_dir, identity))\n",
    "    ]\n",
    "\n",
    "def _split_identities(\n",
    "    identities: list,\n",
    "    train_ratio: float,\n",
    "    val_ratio: float,\n",
    "    test_ratio: float)-> tuple:\n",
    "    \"\"\"Split identities into train/val/test groups\"\"\"\n",
    "    total = len(identities)\n",
    "    train_end = int(train_ratio * total)\n",
    "    val_end = train_end + int(val_ratio * total)\n",
    "    \n",
    "    return (\n",
    "        identities[:train_end],\n",
    "        identities[train_end:val_end],\n",
    "        identities[val_end:]\n",
    "    )\n",
    "\n",
    "def _get_image_paths(identity_path: str) -> list:\n",
    "    \"\"\"Get valid image paths for an identity\"\"\"\n",
    "    return [\n",
    "        os.path.join(identity_path, img)\n",
    "        for img in os.listdir(identity_path)\n",
    "        if img.lower().endswith(('.jpg', '.png', '.jpeg'))\n",
    "    ]\n",
    "\n",
    "def _process_and_save_image(\n",
    "    image_path: str,\n",
    "    output_dir: str,\n",
    "    detector: MTCNN,\n",
    "    target_size: tuple)-> str:\n",
    "    \"\"\"Process single image and save cropped face\"\"\"\n",
    "    try:\n",
    "        # Read and convert image\n",
    "        image = cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Detect faces\n",
    "        faces = detector.detect_faces(image)\n",
    "        if not faces:\n",
    "            return \"skipped\"\n",
    "\n",
    "        # Select best face\n",
    "        best_face = max(faces, key=lambda x: x['confidence'])\n",
    "        x, y, w, h = best_face['box']\n",
    "        \n",
    "        # Crop and resize\n",
    "        face_region = image[y:y+h, x:x+w]\n",
    "        resized_face = cv2.resize(face_region, target_size, interpolation=cv2.INTER_LANCZOS4)\n",
    "        \n",
    "        # Save image\n",
    "        filename = f\"{os.path.splitext(os.path.basename(image_path))[0]}_cropped.jpg\"\n",
    "        output_path = os.path.join(output_dir, filename)\n",
    "        cv2.imwrite(output_path, cv2.cvtColor(resized_face, cv2.COLOR_RGB2BGR))\n",
    "        \n",
    "        return \"saved\"\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Error processing {image_path}: {str(e)}\")\n",
    "        return \"skipped\"\n",
    "\n",
    "def _print_split_stats(split_name: str, valid_count: int, skipped_count: int, save_dir: str) -> None:\n",
    "    \"\"\"Print statistics for processed split\"\"\"\n",
    "    print(f\"\\n{split_name.capitalize()} Set Summary:\")\n",
    "    print(f\"âœ… Successfully saved faces: {valid_count}\")\n",
    "    print(f\"â­ï¸ Skipped images (no face detected/errors): {skipped_count}\")\n",
    "    print(f\"ðŸ“Š Total images processed: {valid_count + skipped_count}\")\n",
    "    print(f\"ðŸ“ Output directory: {os.path.abspath(save_dir)}/{split_name}\\n\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    split_dataset(\n",
    "        root_dir=\"Face Recognition Dataset\",\n",
    "        save_dir=\"processed_data\",\n",
    "        train_ratio=0.8,\n",
    "        val_ratio=0.1,\n",
    "        test_ratio=0.1,\n",
    "        target_size=(224, 224)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267d212c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73943b0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f1a96d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1214dbd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e255651a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6f96b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eea591f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6b8f82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d75d1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c3bb56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604f6af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# triplet loss function\n",
    "# This function computes the triplet loss for a batch of anchor, positive, and negative samples.\n",
    "class TripletLoss(nn.Module):\n",
    "    def __init__(self, margin=0.5):\n",
    "        super(TripletLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, anchor, positive, negative):\n",
    "        # Compute pairwise distances\n",
    "        distance_positive = F.pairwise_distance(anchor, positive)\n",
    "        distance_negative = F.pairwise_distance(anchor, negative)\n",
    "        \n",
    "        # Calculate triplet loss\n",
    "        losses = F.relu(distance_positive - distance_negative + self.margin)\n",
    "        return losses.mean()\n",
    "    \n",
    "class SiameseResNet(nn.Module):\n",
    "    def __init__(self, embedding_dim=256):\n",
    "        super(SiameseResNet, self).__init__()\n",
    "        # Load pretrained ResNet18\n",
    "        self.backbone = torchvision.models.resnet18(weights=\"IMAGENET1K_V1\", progress=True)\n",
    "        \n",
    "        # Replace the final fully connected layer\n",
    "        self.backbone.fc = nn.Linear(512, embedding_dim)  # 512 -> 256\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)\n",
    "    \n",
    "    def print_parameters_count(self):\n",
    "        total_params = sum(p.numel() for p in self.parameters()) / 1e6  # Convert to millions\n",
    "        # Print the number of parameters in millions\n",
    "        print(f\"Total parameters: {total_params:.2f}m\")\n",
    "        \n",
    "model = SiameseResNet(embedding_dim=256)\n",
    "model.print_parameters_count()\n",
    "\n",
    "# pass a sample image through the model\n",
    "dummy_input = torch.randn(1, 3, 224, 224)  # batch size of 1, 3 channels, 224x224 image\n",
    "output = model(dummy_input)\n",
    "output.shape  # should be (1, 256) since we changed the final layer to output 256 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60f3e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "# Define transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(100),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Initialize dataset and dataloader\n",
    "dataset = TripletFaceDataset(root_dir=\"path/to/dataset\", transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Initialize model, loss, and optimizer\n",
    "model = SiameseResNet(embedding_dim=256)\n",
    "criterion = TripletLoss(margin=0.5)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "# Training loop\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(10):\n",
    "    for batch in dataloader:\n",
    "        anchor, positive, negative = batch\n",
    "        anchor, positive, negative = anchor.to(device), positive.to(device), negative.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        anchor_emb = model(anchor)\n",
    "        positive_emb = model(positive)\n",
    "        negative_emb = model(negative)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(anchor_emb, positive_emb, negative_emb)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82942de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3742f799",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
